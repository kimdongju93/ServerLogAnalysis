# 서버 로그 데이터 분석 과제

## 문제 설명

한 웹 서비스의 서버 로그 데이터를 분석하여 성능 문제와 사용 패턴을 파악하려 합니다. 로그 파일에는 각 요청의 시간, HTTP 메소드와 URL 엔드포인트, 응답 상태 코드, 처리 시간(밀리초 단위) 등이 담겨 있습니다. 예를 들어: 2025-05-01T10:15:30Z GET /api/user/list 200 123ms 와 같은 형식입니다. 참가자는 이 로그 데이터를 활용하여 서비스의 병목 구간을 찾아내고, 주요 사용 통계를 도출해야 합니다.

---

## 요구 기능

- **트래픽 분포 분석**: 전체 요청 수 집계, 시간대별(시간/일 단위) 요청량 추이 분석 및 그래프화
- **엔드포인트별 사용 현황**: API별 호출 빈도, 평균 응답시간 등 통계 산출
- **상태 코드 분포**: 2xx/4xx/5xx 비율, 에러 집중 구간/엔드포인트 탐지
- **성능 병목 식별**: 느린 요청/엔드포인트 분석, 90퍼센타일 등 상세 통계
- **추가 인사이트**: 이상 패턴, 비정상 트래픽, 시간대별 특이 현상 등 탐지
- **(선택) 결과 시각화**: matplotlib, seaborn 등으로 그래프 생성
- **(선택) 개선 방안 제안**: 성능 개선, 오류 감소를 위한 제언
- **(선택) LLM 활용**: 로그 요약, 이상 탐지, 자연어 리포트 등
- **(선택) 기타 심화 분석**: 요청 간 상관관계, 사용자 세그먼트 등

---

## 평가 포인트

- **정확한 데이터 처리**: 로그 파싱 및 통계 계산의 정확성
- **통찰력 있는 분석**: 단순 수치 나열이 아닌, 데이터 기반 인사이트 및 개선 아이디어 도출
- **결과의 전달력**: 표, 그래프, 텍스트 등으로 이해하기 쉽게 결과 전달
- **도구 활용 능력**: pandas, matplotlib 등 데이터 분석 도구의 적절한 활용
- **재현성과 코드 품질**: 모듈화, 주석, 하드코딩 최소화, 재현 가능한 코드 구조

---

## 프로젝트 구성

- `generate_sample_log.py`: 샘플 로그 데이터 생성 스크립트
- `server_sample.log`: 샘플 로그 파일 (자동 생성)
- `log_analysis.py`: 로그 분석 및 시각화 메인 스크립트
- `requirements.txt`: 필요 패키지 목록
- `analysis_report.txt`: 분석 요약 리포트 (자동 생성)
- `*.png`: 분석 결과 그래프 이미지

---

## 실행 방법

1. 패키지 설치
   ```bash
   pip install -r requirements.txt
   ```
2. 샘플 로그 생성
   ```bash
   python generate_sample_log.py
   ```
3. 로그 분석 실행
   ```bash
   python log_analysis.py
   ```

---

## 참고 및 문의

- 본 과제는 데이터 분석 및 Python 활용 능력 평가를 위한 예시 과제입니다.
- 문의: [Github Issues](https://github.com/kimdongju93/ServerLogAnalysis/issues) 